{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from scipy.sparse import save_npz, load_npz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.stats import expon, reciprocal\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dhruv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas with tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#! warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['Review', 'overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read processed_train from pickle\n",
    "processed_train = pd.read_pickle(\"processed_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = processed_train[[\"tokenized\", \"tokenized_joined\", \"overall\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = processed_train['tokenized'].tolist()\n",
    "\n",
    "skipgram = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, sg=1, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.save(\"skipgram.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(skipgram.wv.key_to_index) + 1  # Adding 1 to account for padding token\n",
    "embedding_dim = 100  # Or the vector_size you used for your Skip-gram model\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in skipgram.wv.key_to_index.items():\n",
    "    embedding_vector = skipgram.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# save embedding_matrix to pickle\n",
    "with open(\"embedding_matrix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embedding_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26898"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26898"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size-1)  # vocab_size minus 1 to account for padding token\n",
    "tokenizer.fit_on_texts(processed_train['tokenized_joined'].tolist())\n",
    "\n",
    "# Convert texts to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(processed_train['tokenized_joined'].tolist())\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_length = max(len(seq) for seq in sequences)  # Or choose a max_length that suits your dataset\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, trainable=True))\n",
    "model.add(SpatialDropout1D(0.2))  # Helps prevent overfitting by dropping entire 1D feature maps\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5, activation='softmax'))  # For 5 classes\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1735, 100)         2689800   \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  (None, 1735, 100)        0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 1735, 256)        234496    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 128)              164352    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,097,229\n",
      "Trainable params: 3,097,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = processed_train['overall'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y)\n",
    "class_weights_dict = {class_label: weight for class_label, weight in zip(classes, class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_3/bidirectional_3/forward_lstm_5/while/lstm_cell_12/split' defined at (most recent call last):\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n      app.start()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3046, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3101, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3488, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_5700\\247008899.py\", line 1, in <module>\n      history = model.fit(padded_sequences, y, epochs=20, batch_size=32, validation_split=0.2, class_weight=class_weights_dict)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\bidirectional.py\", line 277, in __call__\n      return super().__call__(inputs, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\bidirectional.py\", line 404, in call\n      y = self.forward_layer(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py\", line 553, in __call__\n      return super().__call__(inputs, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py\", line 625, in call\n      last_output, outputs, states = backend.rnn(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\backend.py\", line 5139, in rnn\n      final_outputs = tf.compat.v1.while_loop(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\backend.py\", line 5118, in _step\n      output, new_states = step_function(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py\", line 623, in step\n      return self.cell(inputs, states, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py\", line 295, in call\n      k_i, k_f, k_c, k_o = tf.split(\nNode: 'sequential_3/bidirectional_3/forward_lstm_5/while/lstm_cell_12/split'\nOOM when allocating tensor with shape[256,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_3/bidirectional_3/forward_lstm_5/while/lstm_cell_12/split}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_21340]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_3/bidirectional_3/forward_lstm_5/while/lstm_cell_12/split' defined at (most recent call last):\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n      app.start()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 737, in start\n      self.io_loop.start()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 524, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 418, in dispatch_shell\n      await result\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 758, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 426, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3046, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3101, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3306, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3488, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\dhruv\\AppData\\Local\\Temp\\ipykernel_5700\\247008899.py\", line 1, in <module>\n      history = model.fit(padded_sequences, y, epochs=20, batch_size=32, validation_split=0.2, class_weight=class_weights_dict)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\bidirectional.py\", line 277, in __call__\n      return super().__call__(inputs, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\bidirectional.py\", line 404, in call\n      y = self.forward_layer(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py\", line 553, in __call__\n      return super().__call__(inputs, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py\", line 625, in call\n      last_output, outputs, states = backend.rnn(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\backend.py\", line 5139, in rnn\n      final_outputs = tf.compat.v1.while_loop(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\backend.py\", line 5118, in _step\n      output, new_states = step_function(\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py\", line 623, in step\n      return self.cell(inputs, states, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\dhruv\\anaconda3\\envs\\TF\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py\", line 295, in call\n      k_i, k_f, k_c, k_o = tf.split(\nNode: 'sequential_3/bidirectional_3/forward_lstm_5/while/lstm_cell_12/split'\nOOM when allocating tensor with shape[256,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node sequential_3/bidirectional_3/forward_lstm_5/while/lstm_cell_12/split}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_21340]"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_sequences, y, epochs=20, batch_size=32, validation_split=0.2, class_weight=class_weights_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
